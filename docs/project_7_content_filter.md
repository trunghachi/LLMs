# Project 7: Content Filter

Add a content filter to an LLM endpoint.

## Objective
Block harmful outputs using regex.

## Steps
1. Define harmful keywords/patterns.
2. Integrate with Project 5â€™s endpoint.
3. Test with sample inputs.

## Run
```bash
python src/section_7_ethics/content_filter.py
```
